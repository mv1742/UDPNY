{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable= 'pct_ch_hinc00_16'\n",
    "binary6typ = variable+'_binary'\n",
    "model_name = '4_CENSUS_TW_'+variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Foursquare Data\n",
    "# NYU - CUSP UDP Capstone\n",
    "# Foursquare + NYC Merge by CT \n",
    "## Brief\n",
    "This notebook \n",
    "1. Part I. Data Processing\n",
    "\n",
    "    - import Foursquare data\n",
    "    - imports the Census Tract shapefile\n",
    "    - import Typologies\n",
    "    - merges topologies\n",
    "    - Spatail join by Census Tract\n",
    "    - merges topologies\n",
    "    - map topologies\n",
    "    - map Businesses\n",
    "1. Part II. Data Processing\n",
    "    - Performs a classifcation task on Typologies\n",
    "###  You can refer to https://github.com/mv1742/updny_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Anaconda3-5.0.0-Linux-x86_64/envs/ADS/lib/python3.5/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['linalg', 'Polygon']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import shapely\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt #plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stat\n",
    "#make sure plots are embedded into the notebook\n",
    "%matplotlib inline\n",
    "#import statsmodels.formula.api as smf\n",
    "import itertools\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from sklearn.metrics import silhouette_score\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "from scipy import linalg\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage  # for hierarchical clustering\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "from scipy import linalg\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage  # for hierarchical clustering\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import geopandas as gpd\n",
    "import pylab as pl\n",
    "import io\n",
    "import pylab as pl\n",
    "\n",
    "from geopandas import GeoDataFrame\n",
    "from geopandas.tools import sjoin\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    hasWidgets = True\n",
    "except ImportError:\n",
    "    hasWidgets = False\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "# from sklearn.metrics import precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, classification_report\n",
    "import matplotlib.pylab\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYU - CUSP UDP Capstone\n",
    "# Foursquare + NYC Merge by CT \n",
    "## Brief\n",
    "This notebook \n",
    "- import Foursquare data\n",
    "- imports the Census Tract shapefile\n",
    "- import Typologies\n",
    "- merges topologies\n",
    "- Spatail join by Census Tract\n",
    "- merges topologies\n",
    "- map topologies\n",
    "- map Businesses\n",
    "- outputs a .csv of the results\n",
    "- You can refer to https://github.com/mv1742/updny_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foursquare Data\n",
    "- import Foursquare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Census Tract shapefile\n",
    "- imports the Census Tract shapefile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the directory for shapefiles and set the environmental variable to it\n",
    "\n",
    "#masterdata = os.getenv(\"Master\")\n",
    "#if masterdata is None:\n",
    "#    os.environ[\"Master\"] = \"{}/Capstone/udpny_2\".format(os.getenv(\"HOME\"))\n",
    "#    masterdata = os.getenv(\"Master\")\n",
    "#    print(\"Warning: Master environmental variable not found and set by code, please review!\")\n",
    "#print(\"Master: {}\".format(masterdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the directory for shapefiles and set the environmental variable to it\n",
    "\n",
    "# masterdata = os.getenv(\"Master\")\n",
    "# if masterdata is None:\n",
    "#     os.environ[\"Master\"] = \"{}/Capstone/udpny_2\".format(os.getenv(\"HOME\"))\n",
    "#     masterdata = os.getenv(\"Master\")\n",
    "#     print(\"Warning: Master environmental variable not found and set by code, please review!\")\n",
    "# print(\"Master: {}\".format(masterdata))\n",
    "# def getGeoDataFrameFromShpFileZipUrl(url):\n",
    "#     '''\n",
    "#     This function downloads the zip file, unzips it into the dorectory \n",
    "#     pointed to by PUIdata environment variable. Then it \n",
    "#     reads it into a gepandas dataframe\n",
    "#     '''\n",
    "    \n",
    "#     folderName = 'shape'+ \\\n",
    "#         str(len(os.listdir(os.getenv('TaxiData')))+1)\n",
    "#     os.makedirs(os.getenv('Master') + '/' + folderName)\n",
    "#     urlretrieve(url, \"region.zip\")\n",
    "#     os.system('unzip -d $Master'+'/'+folderName+' region.zip')\n",
    "#     filenames = [f for f in os.listdir(os.getenv('Master') + '/' + folderName) if f.endswith('.shp') ]\n",
    "#     shapeFile = filenames[0]\n",
    "#     shapeFilePath = os.getenv('Master') + '/' + folderName + '/' + shapeFile\n",
    "#     return gpd.GeoDataFrame.from_file(shapeFilePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://planninglabs.carto.com/api/v2/sql?filename=region&q=SELECT%20%2A%20FROM%20region_censustract_v0&format=SHP'\n",
    "# NYCzip = getGeoDataFrameFromShpFileZipUrl(url)\n",
    "NYCzip=gpd.read_file('Censustracts/region.shp')\n",
    "NYCzip.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYCzip.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYCzip.rename(columns={\"geoid\": \"GEOID\"},inplace=True)\n",
    "NYCzip.GEOID = NYCzip.GEOID.astype(int)\n",
    "cols = ['GEOID','geometry']\n",
    "NYCzip = NYCzip.loc[:,cols]\n",
    "#NYCzipgdp.plot(column='GEOID',legend = True)\n",
    "NYCzip.shape\n",
    "NYCzip.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(figsize=(15, 15))\n",
    "NYCzip.plot(column='GEOID',legend = True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typologies\n",
    "- import raw Typology file with Census data 'NY_final_data_for_typologies_1.19.19.csv'\n",
    "- merges Typologies with the new Binary typologies\n",
    "- map topologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Typologiespd=pd.read_csv('NY_final_data_for_typologies_1.19.19.csv')\n",
    "Typologiesgdp = gpd.GeoDataFrame(Typologiespd)\n",
    "len(Typologiesgdp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Typologiesgdp.rename(columns={'geoid': \"GEOID\"},inplace=True)\n",
    "Typologiesgdp.tail()\n",
    "cols_typ = ['GEOID','Type_1.19']\n",
    "print(type(Typologiesgdp.iloc[:,0][0]))\n",
    "#Typologies.rename(columns={'\\ufeffgeoid': \"GEOID\"},inplace=True)\n",
    "#Typologiesgdp.geoid = Typologies.iloc[:,0]\n",
    "Typologiesgdp = Typologiesgdp.loc[:,cols_typ]\n",
    "Typologiesgdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Binaries=pd.read_csv('./Data/NEW_6_BINARIES_ALL.csv')\n",
    "len(Binaries.columns), Binaries.shape\n",
    "Binaries.drop(columns = 'Unnamed: 0', inplace=True)\n",
    "Binaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_typs = ['pct_ch_hinc00_16_binary',\n",
    "            'pct_ch_medhval00_16_binary','pct_ch_medrent00_16_binary','pct_ch_percol00_16_binary','Ongoing_adv_gent',\n",
    "            'gent00_16','gent90_00','Supergent16']\n",
    "for i, column in enumerate(bin_typs):\n",
    "    plt.figure(1)\n",
    "    plt.subplot(4,4,i+1)\n",
    "    Binaries[column].value_counts().plot(kind='bar', figsize = (15,15), title=column)\n",
    "    Binaries[column].value_counts()/Binaries[column].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Typologiesgdp = Typologiesgdp.merge(Binaries, on= 'GEOID')\n",
    "Typologiesgdp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Typologiesgdp.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = NYCzip.merge(Typologiesgdp,on='GEOID')\n",
    "merged.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(merged),merged.shape)\n",
    "mergedgpd = gpd.GeoDataFrame(merged)\n",
    "mergedgpd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD TWITTER DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alltwitterandcensusdata.csv\n",
    "alltwitterandcensusdata = pd.read_csv('Data/alltwitterandcensusdata.csv')\n",
    "alltwitterandcensusdata.rename(columns={'geojoin': \"GEOID\"},inplace=True)\n",
    "alltwitterandcensusdata.drop(columns='Unnamed: 0',inplace=True)\n",
    "print(alltwitterandcensusdata.shape)\n",
    "twittercols = list(alltwitterandcensusdata.columns[:-23])+['distToHighVisitorTract',\\\n",
    "                                                     'distToHighTweetTract','distToMHI']\n",
    "print('twittercols------------------------------')\n",
    "print(list(twittercols))\n",
    "print('alltwitterandcensusdata------------------------------')\n",
    "print(list(alltwitterandcensusdata.columns))\n",
    "twitterdata  = alltwitterandcensusdata.loc[:,twittercols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD CENSUS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Censuspd_TOD = pd.read_stata('Data/UDP_NYC_Variables.dta')\n",
    "Censuspd_TOD.rename(columns={'GEOid2': \"GEOID\"},inplace=True)\n",
    "Censuspd_TOD = Censuspd_TOD.loc[:,['GEOID','TOD']]\n",
    "Censuspd_TOD.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO GEOID\n",
    "Census_data_original_NOGEOID = pd.read_csv('./Data/originalcensusfeatures.csv')\n",
    "Census_data_original_NOGEOID.drop(columns='Unnamed: 0',inplace=True)\n",
    "cols_orig = list(Census_data_original_NOGEOID.columns)\n",
    "print(cols_orig)\n",
    "Census_data_original_NOGEOID.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_orig_noTOD = ['medrent00', 'medhval00', 'percol00', 'per_rent_00', 'pernwh00', 'hinc00', 'carcommuters_00']\n",
    "Census_data_original=pd.read_csv('NY_final_data_for_typologies_1.19.19.csv')\n",
    "Census_data_original.rename(columns={'geoid': \"GEOID\"},inplace=True)\n",
    "Census_data_original = Census_data_original.loc[:,cols_orig_noTOD+['GEOID']]\n",
    "Census_data_original.loc[:,cols_orig_noTOD+['GEOID']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Census_data_new = pd.read_csv('./Data/newcensusfeatures.csv')\n",
    "Census_data_new.rename(columns={'geoid': \"GEOID\"},inplace=True)\n",
    "Census_data_new.drop(columns=['geojoin','Unnamed: 0'],inplace=True)\n",
    "Census_data_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Census_data_original['GEOID'] = pd.to_numeric(Census_data_original['GEOID'])\n",
    "Censuspd_TOD['GEOID'] = pd.to_numeric(Censuspd_TOD['GEOID'])\n",
    "Census_data_new['GEOID']  = pd.to_numeric(Census_data_new['GEOID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Censuspd_TOD, Census_data_original, Census_data_new\n",
    "print(Censuspd_TOD.shape, Census_data_original.shape, Census_data_new.shape)\n",
    "Census_new_temp = Censuspd_TOD.merge(Census_data_original,on='GEOID')\n",
    "print(Census_new_temp.shape)\n",
    "Census_combined = Census_data_new.merge(Census_new_temp,on='GEOID')\n",
    "print(Census_combined.shape)\n",
    "# (Census_combined.GEOID==Census_combined.geojoin).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twitterdata.shape,twitterdata.columns)\n",
    "TW_pd_temp = twitterdata.merge(mergedgpd, on ='GEOID')\n",
    "print(TW_pd_temp.shape,TW_pd_temp.columns[:10])\n",
    "TW_pd_temp = gpd.GeoDataFrame(TW_pd_temp)\n",
    "figure, ax = plt.subplots()\n",
    "TW_pd_temp.plot(column='GEOID',legend = True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twitterdata.shape)\n",
    "Census_FQpd = Census_combined.merge(twitterdata,on='GEOID')\n",
    "print(Census_FQpd.shape)\n",
    "Census_FQpd.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADD GEOMETRY & TYPOLOGIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Census_FQpd.shape,Census_FQpd.columns)\n",
    "Census_FQpd = Census_FQpd.merge(mergedgpd, on ='GEOID')\n",
    "print(Census_FQpd.shape,Census_FQpd.columns[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Census_FQpd.columns[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typologies = ['Type_1.19','pct_ch_percol00_16_binary','pct_ch_hinc00_16_binary',\\\n",
    "           'pct_ch_medhval00_16_binary','pct_ch_medrent00_16_binary','pct_ch_percol00_16_binary','Ongoing_adv_gent',\n",
    "                                        'gent00_16',\n",
    "                                        'gent90_00',\n",
    "                                      'Supergent16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Census_FQpd = gpd.GeoDataFrame(Census_FQpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Census_FQpd.shape)\n",
    "Census_FQpd.dropna(inplace=True)\n",
    "print(Census_FQpd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots()\n",
    "Census_FQpd.plot(column='GEOID',legend = True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Census_FQpd_beforedrops = Census_FQpd.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary6typ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing Data\n",
    "\n",
    "y = Census_FQpd[binary6typ]\n",
    "X_Census_FQpd = Census_FQpd.drop(typologies+['GEOID','geometry'],axis =1).copy()\n",
    "print(X_Census_FQpd.shape)\n",
    "X_Census_FQpd.dropna(inplace=True)\n",
    "print(X_Census_FQpd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Census_FQpd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "models = ['Raw','Scaled','Minmaxed']\n",
    "Xdata_scaled = preprocessing.scale(X_Census_FQpd)\n",
    "# ydata_scaled = preprocessing.scale(y)\n",
    "\n",
    "Xdata_minmaxed = min_max_scaler.fit_transform(X_Census_FQpd)\n",
    "# ydata_minmaxed = min_max_scaler.fit_transform(y)\n",
    "\n",
    "dictx = {}\n",
    "dictx['Raw'] = X_Census_FQpd\n",
    "dictx['Scaled'] = Xdata_scaled\n",
    "dictx['Minmaxed'] = Xdata_minmaxed\n",
    "                    \n",
    "\n",
    "# x = dictx[model][0]\n",
    "# y = dictx[model][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f1_scores = {}\n",
    "model_recall_scores = {}\n",
    "model_precision_scores = {}\n",
    "imp_dict = {}\n",
    "names_dict ={}\n",
    "model = {}\n",
    "modelslist = ['Logit','DT','RF','SVM'] \n",
    "for m in models:\n",
    "    # X_train[m], X_test[m], y_train[m], y_test[m]\n",
    "    dictx[m]\n",
    "    model[m] = {}\n",
    "    model_f1_scores[m] = {}\n",
    "    model_recall_scores[m] ={}\n",
    "    model_precision_scores[m] = {}\n",
    "    imp_dict[m] = {}\n",
    "    names_dict[m] = {}\n",
    "resultset=Census_FQpd_beforedrops.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = {}\n",
    "X_test = {}\n",
    "y_train = {}\n",
    "y_test = {}\n",
    "for m in models:\n",
    "\n",
    "    X_train[m], X_test[m], y_train[m], y_test[m] = train_test_split(dictx[m], y, test_size = 0.3, random_state = 1)    \n",
    "    #x >> dictx[m][0]\n",
    "    # y =>> dictx[m][1]\n",
    "    print(X_train[m].shape, X_test[m].shape, y_train[m].shape, y_test[m].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model\n",
    "def f_importances_neg(coef, names,m):\n",
    "    \n",
    "    imp = coef\n",
    "    print((imp.shape))\n",
    "    imp,names = zip(*sorted(zip(list(imp)[0],names)))\n",
    "    imp = imp[:20]\n",
    "    names = names[:20]\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title('Negative Weights of Logistic Classifier for Model \\''+m+'\\'', size = 10)\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)  \n",
    "    plt.xticks(size = 8)\n",
    "    plt.show()\n",
    "def f_importances_pos(coef, names,m):\n",
    "    imp = coef\n",
    "    print((imp.shape))\n",
    "    imp,names = zip(*sorted(zip(list(imp)[0],names)))\n",
    "    imp = imp[-20:]\n",
    "    names = names[-20:]\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title('Positive Weights of Logistic Classifier for Model \\''+m+'\\'', size = 10)\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "#     plt.xlabel(size=8)\n",
    "    plt.xticks(size = 8)\n",
    "    plt.show()\n",
    "def f_importances_unimp(coef, names,m):\n",
    "    imp = coef\n",
    "    print((imp.shape))\n",
    "    imp,names = zip(*sorted(zip(list(imp)[0],names)))\n",
    "    lenimp_2 = len(imp)//2\n",
    "    imp = imp[lenimp_2-10:lenimp_2+10]\n",
    "    names = names[lenimp_2-10:lenimp_2+10]\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title('Weights of Logistic Classifier for Model \\''+m+'\\'', size = 10)\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.xticks(size = 8)\n",
    "    plt.show()\n",
    "def f_importances_all(coef, names,m):\n",
    "    imp = coef\n",
    "    print((imp.shape))\n",
    "    imp,names = zip(*sorted(zip(list(imp)[0],names)))\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title('Negative Weights of Logistic Classifier for Model \\''+m+'\\'', size = 10)\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.xticks(size = 8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Logistic Reeg\n",
    "imp = {}\n",
    "for m in models:\n",
    "#     dictx[m]\n",
    "#     model[m]\n",
    "# X_train[m], X_test[m], y_train[m], y_test[m]\n",
    "\n",
    "    logit_1 = LogisticRegression(C = 10000)\n",
    "#     print(X_train[m].shape,y_train[m].shape)\n",
    "    logit_1.fit(X_train[m], y_train[m])\n",
    "#     print(logit_1.score(X_test[m],y_test[m]))\n",
    "    model[m]['Logit'] = logit_1.score(X_test[m],y_test[m])\n",
    "#     resultset[m+'_Logit_predicttyp']=logit_1.predict(dictx[m])\n",
    "    y_pred = logit_1.predict(X_test[m])\n",
    "    y_true = y_test[m]\n",
    "    model_f1_scores[m]['Logit'] = f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "    model_recall_scores[m]['Logit'] = recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "    model_precision_scores[m]['Logit'] = precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "    f_importances_pos(logit_1.coef_, np.asarray(list(X_train['Raw'].columns)),m)\n",
    "    f_importances_neg(logit_1.coef_, np.asarray(list(X_train['Raw'].columns)),m)\n",
    "    f_importances_unimp(logit_1.coef_, np.asarray(list(X_train['Raw'].columns)),m)\n",
    "    imp3,names = zip(*sorted(zip(list(logit_1.coef_)[0],np.asarray(list(X_train['Raw'].columns)))))\n",
    "    names_dict[m]['Logit'] = names\n",
    "    imp_dict[m]['Logit'] =  imp3\n",
    "    f_importances_all(logit_1.coef_, np.asarray(list(X_train['Raw'].columns)),m)\n",
    "model_f1_scores\n",
    "pd.DataFrame(imp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureImportancePlot_dt(rf, labels,m):\n",
    "    importances = rf.feature_importances_[:]\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    importances_10 = rf.feature_importances_[:][:10]\n",
    "    indices_10 = np.argsort(importances_10)[::-1]\n",
    "    #std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "    #         axis=0)\n",
    "    pl.figure(figsize=(5,5))\n",
    "    pl.title(\"Feature importances\")\n",
    "    pl.bar(range(indices_10.shape[0]), rf.feature_importances_[indices_10],\n",
    "       color=\"SteelBlue\", #yerr=std[indices]\n",
    "           align=\"center\")\n",
    "    pl.xticks(range(indices_10.shape[0]), np.array(labels)[indices_10], rotation=90)\n",
    "    pl.xlim([-1, indices_10.shape[0]])\n",
    "    pl.show()\n",
    "    print(importances) \n",
    "    \n",
    "    return rf.feature_importances_[indices], np.array(labels)[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed = 5\n",
    "# print(y_train.shape, X_train.shape)\n",
    "for m in models:\n",
    "    #     dictx[m]\n",
    "    #     model[m]\n",
    "\n",
    "    OS = []\n",
    "#     param_grid = {'n_estimators':range(1,11),'max_depth':range(1,11),'max_leaf_nodes':range(2,11)}\n",
    "#     dt=DecisionTreeClassifier()\n",
    "#     gr=GridSearchCV(dt,param_grid=param_grid,scoring='roc_auc')\n",
    "#     ds=gr.fit(X_train[m],y_train[m])\n",
    "\n",
    "    for c in range(5):\n",
    "#         print (ds.best_params_,ds.best_params_['max_depth'],ds.best_params_['max_leaf_nodes'])\n",
    "        dt=DecisionTreeClassifier()\n",
    "        dt = DecisionTreeClassifier(max_depth=3)\n",
    "        dt.fit(X_train[m], y_train[m])\n",
    "        pred=dt.predict_proba(X_test[m])[:,1]\n",
    "        OS.append(dt.score(X_test[m],y_test[m]))\n",
    "    model[m]['DT'] = mean(OS)\n",
    "#     resultset[m+'_DT_predicttyp']=dt.predict(dictx[m])\n",
    "    y_pred = dt.predict(X_test[m])\n",
    "    y_true = y_test[m]\n",
    "    model_f1_scores[m]['DT'] = f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "    model_recall_scores[m]['DT'] = recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "    model_precision_scores[m]['DT'] = precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "    imp_dict[m]['DT'], names_dict[m]['DT'] =  featureImportancePlot_dt(dt, X_Census_FQpd.columns,m)\n",
    "model_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f1_scores['Raw']['DT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultset['actualtyp']=y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureImportancePlot(rf, labels,m):\n",
    "    '''plots feature importance for random forest\n",
    "    rf: the random forest model fit to the data\n",
    "    labels: the names of the features\n",
    "    '''\n",
    "    \n",
    "    importances = rf.feature_importances_[:10]\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "             axis=0)\n",
    "\n",
    "    pl.figure()\n",
    "    pl.title(\"Feature importances for Model\"+ m)\n",
    "    pl.bar(range(indices.shape[0]), rf.feature_importances_[indices],\n",
    "       color=\"SteelBlue\", yerr=std[indices], align=\"center\")\n",
    "    pl.xticks(range(indices.shape[0]), np.array(labels)[indices], rotation=90)\n",
    "    pl.xlim([-1, indices.shape[0]])\n",
    "    pl.show()\n",
    "\n",
    "\n",
    "# def featureImportancePlot(rf, labels,m):\n",
    "#     '''plots feature importance for random forest\n",
    "#     rf: the random forest model fit to the data\n",
    "#     labels: the names of the features\n",
    "#     '''\n",
    "#     importances = rf.feature_importances_\n",
    "#     indices = np.argsort(importances)[::-1]\n",
    "#     importances_10 = rf.feature_importances_[:10]\n",
    "#     indices_10 = np.argsort(importances_10)[::-1]\n",
    "#     std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "#              axis=0)\n",
    "\n",
    "#     pl.figure(figsize=(5,5))\n",
    "#     pl.title(\"Feature importances of Random Forest for Model \"+m)\n",
    "#     pl.bar(range(indices_10.shape[0]), rf.feature_importances_[indices_10],\n",
    "#        color=\"SteelBlue\", yerr=std[indices_10], align=\"center\")\n",
    "#     pl.xticks(range(indices_10.shape[0]), np.array(labels)[indices_10], rotation=90)\n",
    "#     pl.xlim([-1, indices_10.shape[0]])\n",
    "#     pl.show()\n",
    "#     return rf.feature_importances_[indices], np.array(labels)[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for m in models:\n",
    "    #     dictx[m]\n",
    "    print(m)\n",
    "    param_grid = {'max_depth':range(1,11),'n_estimators':range(1,11),'max_leaf_nodes':range(2,11)}\n",
    "    rf=RandomForestClassifier()\n",
    "    gr=GridSearchCV(rf,param_grid=param_grid,scoring='roc_auc')\n",
    "    rs=gr.fit(X_train[m],y_train[m])\n",
    "    OS = []\n",
    "    for c in range(5):\n",
    "        print(rs.best_params_,rs.best_params_['max_depth'],rs.best_params_['max_leaf_nodes'])\n",
    "        rf = RandomForestClassifier(max_depth=rs.best_params_['max_depth'],max_leaf_nodes=rs.best_params_['max_leaf_nodes'])\n",
    "        rf.fit(X_train[m], y_train[m])\n",
    "        pred=rf.predict_proba(X_test[m])[:,1]\n",
    "        OS.append(rf.score(X_test[m],y_test[m]))\n",
    "    model[m]['RF'] = mean(OS)\n",
    "    resultset[m+'_RF_predicttyp']=rf.predict(dictx[m])\n",
    "    y_pred = rf.predict(X_test[m])\n",
    "    y_true = y_test[m]\n",
    "    model_f1_scores[m]['RF'] = f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "    model_recall_scores[m]['RF'] = recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "    model_precision_scores[m]['RF'] = precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "    imp_dict[m]['RF'], names_dict[m]['RF'] =  featureImportancePlot_dt(dt, X_Census_FQpd.columns,m)\n",
    "model_f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the features have changed considerably with the updated binary typology-- users and checkins were the most important features initially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model\n",
    "def f_importances_neg(coef, names,m):\n",
    "    \n",
    "    imp = coef\n",
    "    print((imp.shape))\n",
    "    imp,names = zip(*sorted(zip(list(imp)[0],names)))\n",
    "    imp = imp[:20]\n",
    "    names = names[:20]\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title('Negative Weights of SVM Classifier for Model \\''+m+'\\'', size = 10)\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)  \n",
    "    plt.xticks(size = 8)\n",
    "    plt.show()\n",
    "def f_importances_pos(coef, names,m):\n",
    "    imp = coef\n",
    "    print((imp.shape))\n",
    "    imp,names = zip(*sorted(zip(list(imp)[0],names)))\n",
    "    imp = imp[-20:]\n",
    "    names = names[-20:]\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title('Positive Weights of SVM Classifier for Model \\''+m+'\\'', size = 10)\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "#     plt.xlabel(size=8)\n",
    "    plt.xticks(size = 8)\n",
    "    plt.show()\n",
    "def f_importances_unimp(coef, names,m):\n",
    "    imp = coef\n",
    "    print((imp.shape))\n",
    "    imp,names = zip(*sorted(zip(list(imp)[0],names)))\n",
    "    lenimp_2 = len(imp)//2\n",
    "    imp = imp[lenimp_2-10:lenimp_2+10]\n",
    "    names = names[lenimp_2-10:lenimp_2+10]\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title('Weights of SVM Classifier for Model \\''+m+'\\'', size = 10)\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.xticks(size = 8)\n",
    "    plt.show()\n",
    "def f_importances_all(coef, names,m):\n",
    "    imp = coef\n",
    "    print((imp.shape))\n",
    "    imp,names = zip(*sorted(zip(list(imp)[0],names)))\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.title('Negative Weights of SVM Classifier for Model \\''+m+'\\'', size = 10)\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "#     plt.xlabel(size=8)\n",
    "    plt.xticks(size = 8)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# When C is very small, we are willing to tolerate more mistakes. If C is very big, this\n",
    "# means we hardly tolerate any mistakes. So, we cannot choose a very large C if our data is not\n",
    "# really separable. Let's however choose from a broad range of reasonable options.\n",
    "# param_grid = {'kernel':['linear'],'C':[np.exp(i) for i in np.linspace(-10,10,10)]}\n",
    "for m in models:\n",
    "    print(m)\n",
    "    OS = []\n",
    "    #     dictx[m]\n",
    "        #     model[m]\n",
    "    rr = svm.SVC(gamma='auto')\n",
    "    rr.fit(X_train[m], y_train[m])\n",
    "    correct=1.0*(rr.predict(X_test[m])==np.asarray(y_test[m])).sum()/len(y_test[m])\n",
    "    print(correct)\n",
    "    print(rr.score(X_test[m],y_test[m]))\n",
    "    OS.append(correct)\n",
    "#     resultset[m+'_SVM_predicttyp']=rr.predict(dictx[m])\n",
    "    model[m]['SVM'] = mean(OS)\n",
    "    y_pred = rf.predict(X_test[m])\n",
    "    y_true = y_test[m]\n",
    "    model_f1_scores[m]['SVM'] = f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "    model_recall_scores[m]['SVM'] = recall_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "    model_precision_scores[m]['SVM'] = precision_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "    f_importances_pos(logit_1.coef_, np.asarray(list(X_train['Raw'].columns)),m)\n",
    "    f_importances_neg(logit_1.coef_, np.asarray(list(X_train['Raw'].columns)),m)\n",
    "    f_importances_unimp(logit_1.coef_, np.asarray(list(X_train['Raw'].columns)),m)\n",
    "    imp3,names = zip(*sorted(zip(list(logit_1.coef_)[0],np.asarray(list(X_train['Raw'].columns)))))\n",
    "    names_dict[m]['SVM'] = names\n",
    "    imp_dict[m]['SVM'] = imp3 \n",
    "model_f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Census_FQpd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Census_FQpd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(model_f1_scores)\n",
    "df2 = pd.DataFrame(model_recall_scores)\n",
    "df3 =pd.DataFrame(model_precision_scores)\n",
    "result = pd.concat([df1, df2,df3], axis=1, sort=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('./Results/Scores'+model_name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_pd = pd.DataFrame()\n",
    "counter = {}\n",
    "for m in models:\n",
    "    print(m)\n",
    "    for standarized in modelslist:\n",
    "        for i, weights in enumerate(names_dict[m][standarized]):\n",
    "            standarized_m = str(standarized)+'_'+str(m)\n",
    "            names_pd.loc[standarized_m,weights] = imp_dict[m][standarized][i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in names_pd.index:\n",
    "    print(names_pd.loc[i].idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names_pd.to_csv('./Results/'+model_name+'.csv')\n",
    "names_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can refer to https://github.com/mv1742/updny_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS",
   "language": "python",
   "name": "ads"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
